{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo_utils import PyMongoUtils\n",
    "\n",
    "mongo_obj = PyMongoUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongo_obj.get_database(\"peopledb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['ac-oxbwsep-shard-00-01.fs22oe8.mongodb.net:27017', 'ac-oxbwsep-shard-00-02.fs22oe8.mongodb.net:27017', 'ac-oxbwsep-shard-00-00.fs22oe8.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Cluster0', authsource='admin', replicaset='atlas-smnn5e-shard-0', tls=True), 'peopledb')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db[\"sentiment_analysis\"]  # Create or connect to a collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_mongodb(data):\n",
    "    if isinstance(data, list):\n",
    "        collection.insert_many(data)  # Store multiple documents\n",
    "    else:\n",
    "        collection.insert_one(data)  # Store a single document\n",
    "\n",
    "# Example processed data from Spark\n",
    "sample_data = {\n",
    "    \"text\": \"This product is amazing!\",\n",
    "    \"sentiment\": \"positive\",\n",
    "    \"timestamp\": \"2025-04-02 10:30:00\"\n",
    "}\n",
    "\n",
    "store_in_mongodb(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-O7B27O1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MongoIntegration</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20aa6cf3710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession  \n",
    "\n",
    "# Create a Spark Session in local mode  \n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SentimentAnalysis\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://lapsap:i3WlHcY2fKGs7TUf@cluster0.fs22oe8.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Verify Spark is running  \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `_id`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\types.py:1644\u001b[39m, in \u001b[36m_infer_type\u001b[39m\u001b[34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[39m\n\u001b[32m   1643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1644\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\types.py:1684\u001b[39m, in \u001b[36m_infer_schema\u001b[39m\u001b[34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[39m\n\u001b[32m   1683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1684\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1685\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1686\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33mdata_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1687\u001b[39m     )\n\u001b[32m   1689\u001b[39m fields = []\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `ObjectId`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\types.py:1695\u001b[39m, in \u001b[36m_infer_schema\u001b[39m\u001b[34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1692\u001b[39m     fields.append(\n\u001b[32m   1693\u001b[39m         StructField(\n\u001b[32m   1694\u001b[39m             k,\n\u001b[32m-> \u001b[39m\u001b[32m1695\u001b[39m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1701\u001b[39m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1702\u001b[39m         )\n\u001b[32m   1703\u001b[39m     )\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\types.py:1650\u001b[39m, in \u001b[36m_infer_type\u001b[39m\u001b[34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[39m\n\u001b[32m   1649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1651\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mUNSUPPORTED_DATA_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1652\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33mdata_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1653\u001b[39m     )\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `ObjectId`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Convert Spark DataFrame to MongoDB\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m processed_data = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m processed_data.write.format(\u001b[33m\"\u001b[39m\u001b[33mmongo\u001b[39m\u001b[33m\"\u001b[39m).mode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m).save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:1443\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m).createDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m   1441\u001b[39m         data, schema, samplingRatio, verifySchema\n\u001b[32m   1442\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1483\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromRDD(data.map(prepare), schema, samplingRatio)\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m     rdd, struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:1093\u001b[39m, in \u001b[36mSparkSession._createFromLocal\u001b[39m\u001b[34m(self, data, schema)\u001b[39m\n\u001b[32m   1090\u001b[39m     data = \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m     converter = _create_converter(struct)\n\u001b[32m   1095\u001b[39m     tupled_data: Iterable[Tuple] = \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:955\u001b[39m, in \u001b[36mSparkSession._inferSchemaFromList\u001b[39m\u001b[34m(self, data, names)\u001b[39m\n\u001b[32m    953\u001b[39m infer_array_from_first_element = \u001b[38;5;28mself\u001b[39m._jconf.legacyInferArrayTypeFromFirstElement()\n\u001b[32m    954\u001b[39m prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m schema = \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    970\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    971\u001b[39m         message_parameters={},\n\u001b[32m    972\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:958\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    953\u001b[39m infer_array_from_first_element = \u001b[38;5;28mself\u001b[39m._jconf.legacyInferArrayTypeFromFirstElement()\n\u001b[32m    954\u001b[39m prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n\u001b[32m    955\u001b[39m schema = reduce(\n\u001b[32m    956\u001b[39m     _merge_type,\n\u001b[32m    957\u001b[39m     (\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[32m    966\u001b[39m     ),\n\u001b[32m    967\u001b[39m )\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    970\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    971\u001b[39m         message_parameters={},\n\u001b[32m    972\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\mongomongo\\.venv\\Lib\\site-packages\\pyspark\\sql\\types.py:1705\u001b[39m, in \u001b[36m_infer_schema\u001b[39m\u001b[34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[39m\n\u001b[32m   1692\u001b[39m         fields.append(\n\u001b[32m   1693\u001b[39m             StructField(\n\u001b[32m   1694\u001b[39m                 k,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m             )\n\u001b[32m   1703\u001b[39m         )\n\u001b[32m   1704\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1706\u001b[39m             error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_INFER_TYPE_FOR_FIELD\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1707\u001b[39m             message_parameters={\u001b[33m\"\u001b[39m\u001b[33mfield_name\u001b[39m\u001b[33m\"\u001b[39m: k},\n\u001b[32m   1708\u001b[39m         )\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `_id`."
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrame to MongoDB\n",
    "processed_data = spark.createDataFrame([sample_data])\n",
    "processed_data.write.format(\"mongo\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in collection.find():\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in collection.find({\"sentiment\": \"positive\"}):\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neo4j start\n",
    "pip install py2neo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Defining a Graph Schema\n",
    "Nodes:\n",
    "\n",
    "User (people commenting on social media)\n",
    "\n",
    "Post (the social media post)\n",
    "\n",
    "Entity (brand, product, topic discussed)\n",
    "\n",
    "Relationships:\n",
    "\n",
    "User -> writes -> Post\n",
    "\n",
    "Post -> mentions -> Entity\n",
    "\n",
    "Post -> has_sentiment -> SentimentLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "# Connect to Neo4j\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Create nodes\n",
    "user = Node(\"User\", name=\"Alice\")\n",
    "post = Node(\"Post\", text=\"I love this product!\", timestamp=\"2025-04-02\")\n",
    "entity = Node(\"Entity\", name=\"ProductX\")\n",
    "sentiment = Node(\"Sentiment\", label=\"Positive\")\n",
    "\n",
    "# Create relationships\n",
    "graph.create(Relationship(user, \"WRITES\", post))\n",
    "graph.create(Relationship(post, \"MENTIONS\", entity))\n",
    "graph.create(Relationship(post, \"HAS_SENTIMENT\", sentiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve posts with positive sentiment:\n",
    "MATCH (p:Post)-[:HAS_SENTIMENT]->(s:Sentiment)\n",
    "WHERE s.label = \"Positive\"\n",
    "RETURN p.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most mentioned entity:\n",
    "MATCH (p:Post)-[:MENTIONS]->(e:Entity)\n",
    "RETURN e.name, COUNT(p) AS mentions ORDER BY mentions DESC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_data = list(collection.find())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in mongo_data:\n",
    "    user = Node(\"User\", name=doc.get(\"username\", \"Unknown\"))\n",
    "    post = Node(\"Post\", text=doc[\"text\"], timestamp=doc[\"timestamp\"])\n",
    "    sentiment = Node(\"Sentiment\", label=doc[\"sentiment\"])\n",
    "\n",
    "    graph.create(Relationship(user, \"WROTE\", post))\n",
    "    graph.create(Relationship(post, \"HAS_SENTIMENT\", sentiment))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
