{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fb4ece1e-d3b2-4275-98a1-c2a88868c746",
   "metadata": {},
   "source": [
    "Author: TAN ZHI WEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cb65db-b3f7-4f6b-9b78-f397f25df458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./de-venv/lib/python3.10/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./de-venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./de-venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9a2df5-9497-4dc3-a682-944c0277d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Collecting requests (from vaderSentiment)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->vaderSentiment)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->vaderSentiment)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->vaderSentiment)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->vaderSentiment)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests, vaderSentiment\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 requests-2.32.3 urllib3-2.4.0 vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e953539-2aeb-4624-ad51-be1888611d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dcb742-0944-4695-915e-79d602d41ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdfs\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docopt (from hdfs)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in ./de-venv/lib/python3.10/site-packages (from hdfs) (2.32.3)\n",
      "Requirement already satisfied: six>=1.9.0 in ./de-venv/lib/python3.10/site-packages (from hdfs) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./de-venv/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./de-venv/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./de-venv/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./de-venv/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (2025.1.31)\n",
      "Building wheels for collected packages: hdfs, docopt\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34431 sha256=c23f65b752d5cd1f81c3bbf3de1c8cb205797506c60695880d1f45f29d1d9c49\n",
      "  Stored in directory: /home/student/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13781 sha256=fe8273bfc730e089e95837bdcf00cdd74f4f45f98897b2e4b6291089e931493e\n",
      "  Stored in directory: /home/student/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built hdfs docopt\n",
      "Installing collected packages: docopt, hdfs\n",
      "Successfully installed docopt-0.6.2 hdfs-2.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b27203-8b57-4825-90e3-794da910bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.1.5-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Downloading kafka_python-2.1.5-py2.py3-none-any.whl (285 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55272aac-d61b-4719-b1c7-4d3e395aebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96b16ec-625f-47b6-91ea-df247f86e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataingestion.tweet_fetcher import fetch_tweets\n",
    "from dataingestion.twitter_api import get_user_data\n",
    "from dataingestion.label_data import analyze_tweet_sentiment\n",
    "from dataingestion.kafka_producer import send_tweets_to_kafka\n",
    "from dataingestion.kafka_consumer import run_kafka_consumer\n",
    "from dataingestion.store_to_HDFS import consume_tweets_to_hdfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8ba6a-41cf-4997-b01b-9d12226807ab",
   "metadata": {},
   "source": [
    "# get tweetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bbf3a9c-2703-4f97-a2ba-088c8fd8aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the username:  Malaysiakini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querystring set to: {'username': 'Malaysiakini'}\n",
      "Username: malaysiakini, Rest ID: 18040230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get user input\n",
    "username = input(\"Enter the username: \")\n",
    "\n",
    "# Call the function\n",
    "user_data = get_user_data(username)\n",
    "\n",
    "# Check if we got valid data\n",
    "if user_data[0] and user_data[1]:\n",
    "    print(f\"Username: {user_data[0]}, Rest ID: {user_data[1]}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve user data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4051a6-e587-449d-a589-dc6ce6baa17d",
   "metadata": {},
   "source": [
    "# Tweet fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c7dd57-2111-431d-8d5e-7406c48f8929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 50 tweets for user 22594051\n",
      "Fetched 50 tweets for user 55186601\n",
      "Fetched 50 tweets for user 18040230\n",
      "Fetched 50 tweets for user 61083422\n",
      "Fetched 50 tweets for user 102098902\n",
      "Fetched 50 tweets for user 145550026\n",
      "\n",
      "Successfully fetched data for all users!\n",
      "Total tweets fetched: 300\n",
      "User 22594051 has 50 tweets\n",
      "User 55186601 has 50 tweets\n",
      "User 18040230 has 50 tweets\n",
      "User 61083422 has 50 tweets\n",
      "User 102098902 has 50 tweets\n",
      "User 145550026 has 50 tweets\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define user IDs here in main\n",
    "    user_ids = [\"22594051\", \"55186601\", \"18040230\", \"61083422\", \"102098902\", \"145550026\"]\n",
    "    \n",
    "    # Call the function with the user IDs\n",
    "    tweets_data = fetch_tweets(\n",
    "        user_ids=user_ids,\n",
    "        count_per_user=50,\n",
    "        output_file='tweets_output.csv'\n",
    "    )\n",
    "    \n",
    "    # Example of how to use the returned data\n",
    "    for user_id, tweets in tweets_data.items():\n",
    "        print(f\"User {user_id} has {len(tweets)} tweets\")\n",
    "\n",
    "# Fix the syntax in the main check\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9c49d-0bfc-4aa9-b5a0-00d95c1da2f1",
   "metadata": {},
   "source": [
    "# label data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9358391f-5f64-44f0-8565-bf5b117c8152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution:\n",
      "Sentiment\n",
      "Positive    125\n",
      "Neutral      92\n",
      "Negative     83\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of tweets with sentiment:\n",
      "    User ID      Name  Followers Count  \\\n",
      "0  22594051  The Star          1903972   \n",
      "1  22594051  The Star          1903972   \n",
      "2  22594051  The Star          1903972   \n",
      "3  22594051  The Star          1903972   \n",
      "4  22594051  The Star          1903972   \n",
      "\n",
      "                                               Tweet                Location  \\\n",
      "0  “There’s no scandal, no soap opera, no intrigue.\"  Kuala Lumpur, Malaysia   \n",
      "1  This is following the implementation of the Wa...  Kuala Lumpur, Malaysia   \n",
      "2  Defence Minister Ng Eng Hen said leadership su...  Kuala Lumpur, Malaysia   \n",
      "3  Tony, Simon and Raymond's grandfather founded ...  Kuala Lumpur, Malaysia   \n",
      "4  Since the US President slapped a 25% tariff on...  Kuala Lumpur, Malaysia   \n",
      "\n",
      "                             Time  Friends Count Sentiment  \n",
      "0  Fri Apr 18 08:35:13 +0000 2025            274  Negative  \n",
      "1  Fri Apr 18 08:25:15 +0000 2025            274   Neutral  \n",
      "2  Fri Apr 18 08:20:12 +0000 2025            274  Positive  \n",
      "3  Fri Apr 18 08:15:12 +0000 2025            274  Positive  \n",
      "4  Fri Apr 18 08:10:12 +0000 2025            274   Neutral  \n",
      "\n",
      "Sentiment distribution by user:\n",
      "Sentiment  Negative  Neutral  Positive\n",
      "User ID                               \n",
      "18040230         27       11        12\n",
      "22594051         10       19        21\n",
      "55186601         15       11        24\n",
      "61083422         11       11        28\n",
      "102098902        11       31         8\n",
      "145550026         9        9        32\n",
      "\n",
      "Percentage of sentiment by user:\n",
      "Sentiment  Negative  Neutral  Positive\n",
      "User ID                               \n",
      "18040230       54.0     22.0      24.0\n",
      "22594051       20.0     38.0      42.0\n",
      "55186601       30.0     22.0      48.0\n",
      "61083422       22.0     22.0      56.0\n",
      "102098902      22.0     62.0      16.0\n",
      "145550026      18.0     18.0      64.0\n",
      "\n",
      "Results saved to tweets_output_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "analyzed_data = analyze_tweet_sentiment('tweets_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52548f-a425-4c7b-b792-bb9dc6f71023",
   "metadata": {},
   "source": [
    "# kafka producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616d70a7-487c-4527-b9e7-3cf2dccf722e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1 data: {'user_id': 22594051, 'name': 'The Star', 'followers_count': 1903972, 'tweet_text': '“There’s no scandal, no soap opera, no intrigue.\"', 'location': 'Kuala Lumpur, Malaysia', 'created_at': 'Fri Apr 18 08:35:13 +0000 2025', 'friends_count': 274, 'sentiment': 'Negative'}\n",
      "Sent to topic: MalaysiaNewsTopic\n",
      "Tweet 2 data: {'user_id': 22594051, 'name': 'The Star', 'followers_count': 1903972, 'tweet_text': 'This is following the implementation of the Water Quality Conservation Programme in Dec 2024.', 'location': 'Kuala Lumpur, Malaysia', 'created_at': 'Fri Apr 18 08:25:15 +0000 2025', 'friends_count': 274, 'sentiment': 'Neutral'}\n",
      "Sent to topic: MalaysiaNewsTopic\n",
      "Tweet 3 data: {'user_id': 22594051, 'name': 'The Star', 'followers_count': 1903972, 'tweet_text': 'Defence Minister Ng Eng Hen said leadership succession is a cardinal strength of the party and that it was time for him to step down to make way for new candidates.', 'location': 'Kuala Lumpur, Malaysia', 'created_at': 'Fri Apr 18 08:20:12 +0000 2025', 'friends_count': 274, 'sentiment': 'Positive'}\n",
      "Sent to topic: MalaysiaNewsTopic\n",
      "All tweets have been sent to Kafka\n",
      "Successfully processed 300 tweets\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dataingestion.kafka_producer import send_tweets_to_kafka\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to call the send_tweets_to_kafka function.\n",
    "    \"\"\"\n",
    "    # Define parameters directly in the code\n",
    "    csv_file_path = 'tweets_output_with_sentiment.csv'\n",
    "    kafka_servers = ['localhost:9092']\n",
    "    verbose = True\n",
    "    \n",
    "    # Validate CSV file path\n",
    "    csv_path = Path(csv_file_path)\n",
    "    if not csv_path.exists():\n",
    "        print(f\"Error: CSV file '{csv_file_path}' does not exist\")\n",
    "        return 1\n",
    "    \n",
    "    try:\n",
    "        # Process the tweets\n",
    "        tweet_count = send_tweets_to_kafka(\n",
    "            csv_file=csv_file_path,\n",
    "            bootstrap_servers=kafka_servers,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully processed {tweet_count} tweets\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tweets: {e}\")\n",
    "        return 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fea5e-99c8-482f-b56c-8af98c289de6",
   "metadata": {},
   "source": [
    "# kafka consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3494fd25-7e25-48a5-af98-1723896bfe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------#\n",
      "#   Kafka Tweet Consumer Tool            #\n",
      "#----------------------------------------#\n",
      "Available Topics:\n",
      "1. PoliticsNewsTopic\n",
      "2. BusinessNewsTopic\n",
      "3. SportsNewsTopic\n",
      "4. EntertainmentNewsTopic\n",
      "5. MalaysiaNewsTopic\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the number of the topic you want to consume from:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Format Options:\n",
      "1. Detailed - Show each field on a separate line\n",
      "2. Table - Show messages in a table format\n",
      "3. Compact - Show messages in a single line each\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Select output format (1-3):  1\n",
      "\n",
      "Enter maximum number of messages to display (default: 5):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consuming messages from MalaysiaNewsTopic...\n",
      "\n",
      "Message 1:\n",
      "  user_id: 22594051\n",
      "  name: The Star\n",
      "  followers_count: 1903972\n",
      "  tweet_text: “There’s no scandal, no soap opera, no intrigue.\"\n",
      "  location: Kuala Lumpur, Malaysia\n",
      "  created_at: Fri Apr 18 08:35:13 +0000 2025\n",
      "  friends_count: 274\n",
      "  sentiment: Negative\n",
      "\n",
      "Message 2:\n",
      "  user_id: 22594051\n",
      "  name: The Star\n",
      "  followers_count: 1903972\n",
      "  tweet_text: This is following the implementation of the Water Quality Conservation Programme in Dec 2024.\n",
      "  location: Kuala Lumpur, Malaysia\n",
      "  created_at: Fri Apr 18 08:25:15 +0000 2025\n",
      "  friends_count: 274\n",
      "  sentiment: Neutral\n",
      "\n",
      "Message 3:\n",
      "  user_id: 22594051\n",
      "  name: The Star\n",
      "  followers_count: 1903972\n",
      "  tweet_text: Defence Minister Ng Eng Hen said leadership succession is a cardinal strength of the party and that it was time for him to step down to make way for new candidates.\n",
      "  location: Kuala Lumpur, Malaysia\n",
      "  created_at: Fri Apr 18 08:20:12 +0000 2025\n",
      "  friends_count: 274\n",
      "  sentiment: Positive\n",
      "\n",
      "Message 4:\n",
      "  user_id: 22594051\n",
      "  name: The Star\n",
      "  followers_count: 1903972\n",
      "  tweet_text: Tony, Simon and Raymond's grandfather founded the business in 1920, and these brothers take pride in continuing the family legacy.\n",
      "  location: Kuala Lumpur, Malaysia\n",
      "  created_at: Fri Apr 18 08:15:12 +0000 2025\n",
      "  friends_count: 274\n",
      "  sentiment: Positive\n",
      "\n",
      "Message 5:\n",
      "  user_id: 22594051\n",
      "  name: The Star\n",
      "  followers_count: 1903972\n",
      "  tweet_text: Since the US President slapped a 25% tariff on car imports, conversations in Kanda’s bars, shops and municipal offices have revolved around one question: what happens to us now?\n",
      "  location: Kuala Lumpur, Malaysia\n",
      "  created_at: Fri Apr 18 08:10:12 +0000 2025\n",
      "  friends_count: 274\n",
      "  sentiment: Neutral\n",
      "\n",
      "Showing first 5 messages. 295 more messages available.\n",
      "\n",
      "✅ Successfully consumed 5 messages from MalaysiaNewsTopic.\n",
      "\n",
      "=== Fields Found in Messages ===\n",
      "  - created_at: Present in 5/5 messages (100.0%)\n",
      "  - followers_count: Present in 5/5 messages (100.0%)\n",
      "  - friends_count: Present in 5/5 messages (100.0%)\n",
      "  - location: Present in 5/5 messages (100.0%)\n",
      "  - name: Present in 5/5 messages (100.0%)\n",
      "  - sentiment: Present in 5/5 messages (100.0%)\n",
      "  - tweet_text: Present in 5/5 messages (100.0%)\n",
      "  - user_id: Present in 5/5 messages (100.0%)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to export these messages to CSV? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kafka consumer operation completed.\n"
     ]
    }
   ],
   "source": [
    "from dataingestion.kafka_consumer import run_kafka_consumer\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point of the application\"\"\"\n",
    "    run_kafka_consumer()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce7e07-4f3d-4789-a38c-b420d956e669",
   "metadata": {},
   "source": [
    "# store to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec0553ab-1aeb-4947-bb34-f8fb32255d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to consume tweets and save to /user/hduser/raw_tweets/tweets_20250418_164452.json\n",
      "Saved 10 tweets to HDFS\n",
      "Saved 20 tweets to HDFS\n",
      "Saved 30 tweets to HDFS\n",
      "Saved 40 tweets to HDFS\n",
      "Saved 50 tweets to HDFS\n",
      "Saved 60 tweets to HDFS\n",
      "Saved 70 tweets to HDFS\n",
      "Saved 80 tweets to HDFS\n",
      "Saved 90 tweets to HDFS\n",
      "Saved 100 tweets to HDFS\n",
      "Saved 110 tweets to HDFS\n",
      "Saved 120 tweets to HDFS\n",
      "Saved 130 tweets to HDFS\n",
      "Saved 140 tweets to HDFS\n",
      "Saved 150 tweets to HDFS\n",
      "Saved 160 tweets to HDFS\n",
      "Saved 170 tweets to HDFS\n",
      "Saved 180 tweets to HDFS\n",
      "Saved 190 tweets to HDFS\n",
      "Saved 200 tweets to HDFS\n",
      "Saved 210 tweets to HDFS\n",
      "Saved 220 tweets to HDFS\n",
      "Saved 230 tweets to HDFS\n",
      "Saved 240 tweets to HDFS\n",
      "Saved 250 tweets to HDFS\n",
      "Saved 260 tweets to HDFS\n",
      "Saved 270 tweets to HDFS\n",
      "Saved 280 tweets to HDFS\n",
      "Saved 290 tweets to HDFS\n",
      "Saved 300 tweets to HDFS\n",
      "Reached limit of 300 tweets\n",
      "Consumer closed. Saved 300 tweets to HDFS.\n",
      "Successfully verified file exists: /user/hduser/raw_tweets/tweets_20250418_164452.json\n",
      "Run 'hadoop fs -cat /user/hduser/raw_tweets/tweets_20250418_164452.json' to view the content\n",
      "To check structure: hadoop fs -cat /user/hduser/raw_tweets/tweets_20250418_164452.json | head -20\n"
     ]
    }
   ],
   "source": [
    "from dataingestion.store_to_HDFS import consume_tweets_to_hdfs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consume_tweets_to_hdfs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad5de49-1d16-460f-b291-7f840ec2e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check availability of data\n",
    "# hadoop fs -ls /user/hduser/raw_tweets/  \n",
    "# hadoop fs -cat /user/hduser/raw_tweets/tweets_20250418_163452.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2695c-2ab7-484d-9489-c5cb80094bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a68189-c86f-4f48-9834-f3165997e75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
