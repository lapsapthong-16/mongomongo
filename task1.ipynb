{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf35e53-7b5e-404c-b0b9-dd1476d364db",
   "metadata": {},
   "source": [
    "# To install before you run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2701b6-ffe5-4afc-9ca6-38bb4b6673fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./de-venv/lib/python3.10/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./de-venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./de-venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be83817f-4664-4355-80bc-e7bc037c25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Collecting requests (from vaderSentiment)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->vaderSentiment)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->vaderSentiment)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->vaderSentiment)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->vaderSentiment)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests, vaderSentiment\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 requests-2.32.3 urllib3-2.3.0 vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89387922-f5f1-4ad4-aa41-24171e753078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.1.5-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Downloading kafka_python-2.1.5-py2.py3-none-any.whl (285 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af07f09c-9c5f-49d2-8092-710b2bb33e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d81346-a24a-47cc-9e59-6dfe7107106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdfs\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docopt (from hdfs)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in ./de-venv/lib/python3.10/site-packages (from hdfs) (2.32.3)\n",
      "Requirement already satisfied: six>=1.9.0 in ./de-venv/lib/python3.10/site-packages (from hdfs) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./de-venv/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./de-venv/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./de-venv/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./de-venv/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (2025.1.31)\n",
      "Building wheels for collected packages: hdfs, docopt\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34431 sha256=c25a928728b42eb48cf988998d25b13469c33d5baec75d0d82776fc193572361\n",
      "  Stored in directory: /home/student/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13781 sha256=fc820c67010e99b2a365a8e5f549195291e1522ee10e8493b3a856e7f784db5e\n",
      "  Stored in directory: /home/student/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built hdfs docopt\n",
      "Installing collected packages: docopt, hdfs\n",
      "Successfully installed docopt-0.6.2 hdfs-2.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f001ca-539c-4695-918c-69e57ac9b750",
   "metadata": {},
   "source": [
    "# Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0f3080-7095-4478-8c72-30879573f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re  # Import the regular expressions module\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import uuid\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import KafkaProducer\n",
    "from hdfs import InsecureClient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe3357-736e-4753-8317-17411b9e4ddc",
   "metadata": {},
   "source": [
    "# Get userID based on twitter user name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a4dfcc-17be-4bcd-ab3c-4fe10d56548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the username:  The Star\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querystring set to: {'username': 'The Star'}\n",
      "Username: , Rest ID: \n"
     ]
    }
   ],
   "source": [
    "url = \"https://twitter241.p.rapidapi.com/user\"\n",
    "\n",
    "def get_user_input():\n",
    "    return input(\"Enter the username: \")\n",
    "\n",
    "def main():\n",
    "    # Get user input\n",
    "    username = get_user_input()\n",
    "\n",
    "    # Set the querystring with the user input\n",
    "    querystring = {\"username\": username}\n",
    "    print(f\"Querystring set to: {querystring}\")\n",
    "\n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": \"6edb283a6cmshee06689689cfd50p13b4c8jsnf64bdb417da6\",\n",
    "        \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Navigate through the JSON structure to extract username and rest_id\n",
    "        user_data = data.get(\"result\", {}).get(\"data\", {}).get(\"user\", {}).get(\"result\", {})\n",
    "        username = user_data.get(\"legacy\", {}).get(\"screen_name\", \"\")\n",
    "        rest_id = user_data.get(\"rest_id\", \"\")\n",
    "        \n",
    "        # Print the extracted username and rest_id\n",
    "        print(f\"Username: {username}, Rest ID: {rest_id}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0dce21-3a54-42b8-afd2-161b8b5b6d10",
   "metadata": {},
   "source": [
    "# Check is every data exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70154e03-833f-4c53-a683-484f75fef1d8",
   "metadata": {},
   "source": [
    "# List of user ID and save content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9737d992-c3ad-4f2d-9e7b-71f31db928f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 22594051 The Star\n",
    "## 55186601 NST_Online\n",
    "## 18040230 malaysiakini\n",
    "## 61083422 theSundaily\n",
    "## 102098902 Free Malaysia Today \n",
    "## 145550026 Herald Malaysia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71b86cb-f234-4a0c-bcab-0e457f9c2a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 50 tweets for user 22594051\n",
      "Fetched 50 tweets for user 55186601\n",
      "Fetched 50 tweets for user 18040230\n",
      "Fetched 50 tweets for user 61083422\n",
      "Fetched 50 tweets for user 102098902\n",
      "Fetched 50 tweets for user 145550026\n",
      "\n",
      "Successfully fetched data for all users!\n",
      "Total tweets fetched: 300\n"
     ]
    }
   ],
   "source": [
    "url = \"https://twitter241.p.rapidapi.com/user-tweets\"\n",
    "user_ids = [\"22594051\", \"55186601\", \"18040230\", \"61083422\", \"102098902\", \"145550026\"]  # List of user IDs\n",
    "count_per_user = 50  \n",
    "headers = {\n",
    "    \"x-rapidapi-key\": \"6edb283a6cmshee06689689cfd50p13b4c8jsnf64bdb417da6\",\n",
    "    \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "}\n",
    "total_tweets_fetched = 0\n",
    "all_tweets_by_user = {}\n",
    "\n",
    "# Open a CSV file to write the output\n",
    "with open('tweets_output.csv', mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    # Write the header\n",
    "    csvwriter.writerow(['User ID', 'Name', 'Followers Count', 'Tweet'])\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        # We'll potentially make multiple requests with pagination to get more tweets\n",
    "        cursor = None\n",
    "        tweets_for_this_user = 0\n",
    "        full_texts_for_user = []\n",
    "        \n",
    "        # Make multiple requests until we reach our desired count or run out of tweets\n",
    "        while tweets_for_this_user < count_per_user:\n",
    "            # Add cursor to querystring if we have one\n",
    "            if cursor:\n",
    "                querystring = {\"user\": user_id, \"count\": \"20\", \"cursor\": cursor}\n",
    "            else:\n",
    "                querystring = {\"user\": user_id, \"count\": \"20\"}\n",
    "                \n",
    "            response = requests.get(url, headers=headers, params=querystring)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "                \n",
    "                # Initialize variables for this batch\n",
    "                batch_tweets = 0\n",
    "                next_cursor = None\n",
    "                \n",
    "                # Extract tweets and look for cursor for pagination\n",
    "                instructions = data.get(\"result\", {}).get(\"timeline\", {}).get(\"instructions\", [])\n",
    "                \n",
    "                for instruction in instructions:\n",
    "                    if instruction.get(\"type\") == \"TimelineAddEntries\":\n",
    "                        entries = instruction.get(\"entries\", [])\n",
    "                        for entry in entries:\n",
    "                            # Check if this is a cursor entry\n",
    "                            if entry.get(\"entryId\", \"\").startswith(\"cursor-bottom\"):\n",
    "                                content = entry.get(\"content\", {})\n",
    "                                if content.get(\"cursorType\") == \"Bottom\":\n",
    "                                    next_cursor = content.get(\"value\")\n",
    "                                continue\n",
    "                                \n",
    "                            tweet_result = entry.get(\"content\", {}).get(\"itemContent\", {}).get(\"tweet_results\", {}).get(\"result\", {})\n",
    "                            full_text = tweet_result.get(\"legacy\", {}).get(\"full_text\", \"\")\n",
    "                            user_name = tweet_result.get(\"core\", {}).get(\"user_results\", {}).get(\"result\", {}).get(\"legacy\", {}).get(\"name\", \"\")\n",
    "                            followers_count = tweet_result.get(\"core\", {}).get(\"user_results\", {}).get(\"result\", {}).get(\"legacy\", {}).get(\"followers_count\", 0)\n",
    "                            \n",
    "                            # Limit followers count to 10 digits\n",
    "                            if followers_count > 9999999999:\n",
    "                                followers_count = 9999999999\n",
    "                            \n",
    "                            if full_text:\n",
    "                                # Remove URLs from the full_text and strip whitespace\n",
    "                                full_text = re.sub(r'http\\S+', '', full_text).strip()\n",
    "                                # Append the full_text to the list\n",
    "                                full_texts_for_user.append(full_text)\n",
    "                                # Write to CSV\n",
    "                                csvwriter.writerow([user_id, user_name, followers_count, full_text])\n",
    "                                batch_tweets += 1\n",
    "                                # Break if we've reached our limit\n",
    "                                if len(full_texts_for_user) >= count_per_user:\n",
    "                                    break\n",
    "                \n",
    "                tweets_for_this_user += batch_tweets\n",
    "                \n",
    "                # If we didn't get any tweets in this batch or no next cursor, break\n",
    "                if batch_tweets == 0 or not next_cursor or len(full_texts_for_user) >= count_per_user:\n",
    "                    break\n",
    "                    \n",
    "                # Set the cursor for the next request\n",
    "                cursor = next_cursor\n",
    "                \n",
    "                # Add a small delay to avoid rate limiting (optional)\n",
    "                # import time\n",
    "                # time.sleep(1)\n",
    "            else:\n",
    "                print(f\"Error for user {user_id}: {response.status_code}\")\n",
    "                print(response.text)\n",
    "                break\n",
    "        \n",
    "        # Store tweets for this user\n",
    "        all_tweets_by_user[user_id] = full_texts_for_user\n",
    "        total_tweets_fetched += len(full_texts_for_user)\n",
    "        \n",
    "        print(f\"Fetched {len(full_texts_for_user)} tweets for user {user_id}\")\n",
    "\n",
    "# Check if we have data for all users\n",
    "if len(all_tweets_by_user) == len(user_ids):\n",
    "    print(\"\\nSuccessfully fetched data for all users!\")\n",
    "else:\n",
    "    print(f\"\\nFetched data for {len(all_tweets_by_user)} out of {len(user_ids)} users\")\n",
    "print(f\"Total tweets fetched: {total_tweets_fetched}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4411892d-fc21-4523-9926-0651effa45d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 50 tweets for user 22594051\n"
     ]
    }
   ],
   "source": [
    "url = \"https://twitter241.p.rapidapi.com/user-tweets\"\n",
    "user_ids = [\"22594051\", \"55186601\", \"18040230\", \"61083422\", \"102098902\", \"145550026\"]  # List of user IDs\n",
    "count_per_user = 50  \n",
    "headers = {\n",
    "    \"x-rapidapi-key\": \"6edb283a6cmshee06689689cfd50p13b4c8jsnf64bdb417da6\",\n",
    "    \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "}\n",
    "total_tweets_fetched = 0\n",
    "all_tweets_by_user = {}\n",
    "\n",
    "# Open a CSV file to write the output\n",
    "with open('tweets_output.csv', mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    # Write the header\n",
    "    csvwriter.writerow(['User ID', 'Name', 'Followers Count', 'Tweet', 'Location', 'Tweet Time', 'Friends Count'])\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        # We'll potentially make multiple requests with pagination to get more tweets\n",
    "        cursor = None\n",
    "        tweets_for_this_user = 0\n",
    "        full_texts_for_user = []\n",
    "        \n",
    "        # Make multiple requests until we reach our desired count or run out of tweets\n",
    "        while tweets_for_this_user < count_per_user:\n",
    "            # Add cursor to querystring if we have one\n",
    "            if cursor:\n",
    "                querystring = {\"user\": user_id, \"count\": \"20\", \"cursor\": cursor}\n",
    "            else:\n",
    "                querystring = {\"user\": user_id, \"count\": \"20\"}\n",
    "                \n",
    "            response = requests.get(url, headers=headers, params=querystring)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "                \n",
    "                # Initialize variables for this batch\n",
    "                batch_tweets = 0\n",
    "                next_cursor = None\n",
    "                \n",
    "                # Extract tweets and look for cursor for pagination\n",
    "                instructions = data.get(\"result\", {}).get(\"timeline\", {}).get(\"instructions\", [])\n",
    "                \n",
    "                for instruction in instructions:\n",
    "                    if instruction.get(\"type\") == \"TimelineAddEntries\":\n",
    "                        entries = instruction.get(\"entries\", [])\n",
    "                        for entry in entries:\n",
    "                            # Check if this is a cursor entry\n",
    "                            if entry.get(\"entryId\", \"\").startswith(\"cursor-bottom\"):\n",
    "                                content = entry.get(\"content\", {})\n",
    "                                if content.get(\"cursorType\") == \"Bottom\":\n",
    "                                    next_cursor = content.get(\"value\")\n",
    "                                continue\n",
    "                                \n",
    "                            tweet_result = entry.get(\"content\", {}).get(\"itemContent\", {}).get(\"tweet_results\", {}).get(\"result\", {})\n",
    "                            full_text = tweet_result.get(\"legacy\", {}).get(\"full_text\", \"\")\n",
    "                            tweet_time = tweet_result.get(\"legacy\", {}).get(\"created_at\", \"\")\n",
    "                            \n",
    "                            user_info = tweet_result.get(\"core\", {}).get(\"user_results\", {}).get(\"result\", {})\n",
    "                            user_legacy = user_info.get(\"legacy\", {})\n",
    "                            user_name = user_legacy.get(\"name\", \"\")\n",
    "                            followers_count = user_legacy.get(\"followers_count\", 0)\n",
    "                            location = user_legacy.get(\"location\", \"\")\n",
    "                            friends_count = user_legacy.get(\"friends_count\", 0)\n",
    "\n",
    "                            \n",
    "                            # Limit followers count to 10 digits\n",
    "                            if followers_count > 9999999999:\n",
    "                                followers_count = 9999999999\n",
    "                            \n",
    "                            if full_text:\n",
    "                                # Remove URLs from the full_text and strip whitespace\n",
    "                                full_text = re.sub(r'http\\S+', '', full_text).strip()\n",
    "                                # Append the full_text to the list\n",
    "                                full_texts_for_user.append(full_text)\n",
    "                                # Write to CSV\n",
    "                                csvwriter.writerow([user_id, user_name, followers_count, full_text, location, tweet_time, friends_count])\n",
    "                                batch_tweets += 1\n",
    "                                # Break if we've reached our limit\n",
    "                                if len(full_texts_for_user) >= count_per_user:\n",
    "                                    break\n",
    "                \n",
    "                tweets_for_this_user += batch_tweets\n",
    "                \n",
    "                # If we didn't get any tweets in this batch or no next cursor, break\n",
    "                if batch_tweets == 0 or not next_cursor or len(full_texts_for_user) >= count_per_user:\n",
    "                    break\n",
    "                    \n",
    "                # Set the cursor for the next request\n",
    "                cursor = next_cursor\n",
    "                \n",
    "                # Add a small delay to avoid rate limiting (optional)\n",
    "                # import time\n",
    "                # time.sleep(1)\n",
    "            else:\n",
    "                print(f\"Error for user {user_id}: {response.status_code}\")\n",
    "                print(response.text)\n",
    "                break\n",
    "        \n",
    "        # Store tweets for this user\n",
    "        all_tweets_by_user[user_id] = full_texts_for_user\n",
    "        total_tweets_fetched += len(full_texts_for_user)\n",
    "        \n",
    "        print(f\"Fetched {len(full_texts_for_user)} tweets for user {user_id}\")\n",
    "\n",
    "# Check if we have data for all users\n",
    "if len(all_tweets_by_user) == len(user_ids):\n",
    "    print(\"\\nSuccessfully fetched data for all users!\")\n",
    "else:\n",
    "    print(f\"\\nFetched data for {len(all_tweets_by_user)} out of {len(user_ids)} users\")\n",
    "print(f\"Total tweets fetched: {total_tweets_fetched}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e4364-d8ab-4b74-9be6-bb4d81173fdc",
   "metadata": {},
   "source": [
    "# Use vader to label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e08e8-6453-4eb2-af8c-58f666441b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('tweets_output.csv')\n",
    "\n",
    "# Initialize the Vader SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to classify sentiment\n",
    "def classify_sentiment(tweet):\n",
    "    # Check if the tweet is a string\n",
    "    if pd.isna(tweet) or not isinstance(tweet, str):\n",
    "        return 'Unknown'  # Handle NaN or non-string values\n",
    "    \n",
    "    sentiment_score = analyzer.polarity_scores(tweet)\n",
    "    compound_score = sentiment_score['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis on the 'Tweet' column\n",
    "df['Sentiment'] = df['Tweet'].apply(classify_sentiment)\n",
    "\n",
    "# Count the number of tweets with each sentiment\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "print(\"Sentiment distribution:\")\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Display the first few rows of the dataframe with sentiment\n",
    "print(\"\\nSample of tweets with sentiment:\")\n",
    "print(df.head())\n",
    "\n",
    "# Calculate sentiment distribution by user\n",
    "print(\"\\nSentiment distribution by user:\")\n",
    "sentiment_by_user = df.groupby(['User ID', 'Sentiment']).size().unstack(fill_value=0)\n",
    "print(sentiment_by_user)\n",
    "\n",
    "# Calculate the percentage of positive, negative, and neutral tweets for each user\n",
    "sentiment_percentage = sentiment_by_user.div(sentiment_by_user.sum(axis=1), axis=0) * 100\n",
    "print(\"\\nPercentage of sentiment by user:\")\n",
    "print(sentiment_percentage.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee92541-d6c9-44b6-a527-acd9de7af3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f64552-d495-465a-8297-99f7c5da4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a new CSV\n",
    "df.to_csv('tweets_output_with_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99926b45-14bb-42c5-9266-f8184caec280",
   "metadata": {},
   "source": [
    "# write Kafka producer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138611f-cefc-47b2-b2a7-085ec3fe1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define CATEGORY_KEYWORDS dictionary\n",
    "CATEGORY_KEYWORDS = {\n",
    "    'PoliticsNewsTopic': [\n",
    "        'parliament', 'minister', 'government', 'election', 'policy', 'vote', \n",
    "        'cabinet', 'PM', 'democracy', 'corruption', 'political', 'politician',\n",
    "        'law', 'bill', 'constitution', 'amendment', 'opposition', 'campaign',\n",
    "        'UMNO', 'PAS', 'PKR', 'DAP', 'Bersatu', 'Pakatan', 'Barisan', 'budget'\n",
    "    ],\n",
    "    'BusinessNewsTopic': [\n",
    "        'economy', 'market', 'stock', 'investment', 'company', 'business', \n",
    "        'trade', 'finance', 'bank', 'ringgit', 'profit', 'revenue', 'CEO',\n",
    "        'entrepreneur', 'startup', 'commerce', 'industry', 'economic', \n",
    "        'inflation', 'recession', 'growth', 'GST', 'tax', 'BURSA', 'FDI'\n",
    "    ],\n",
    "    'SportsNewsTopic': [\n",
    "        'football', 'badminton', 'hockey', 'athlete', 'tournament', 'championship',\n",
    "        'league', 'match', 'player', 'coach', 'team', 'sport', 'medal', 'win',\n",
    "        'game', 'score', 'FIFA', 'Olympic', 'Petronas', 'stadium', 'final',\n",
    "        'competition', 'record', 'JDT', 'Selangor', 'Perak', 'Malaysia Super League'\n",
    "    ],\n",
    "    'EntertainmentNewsTopic': [\n",
    "        'movie', 'music', 'concert', 'celebrity', 'actor', 'actress', 'film',\n",
    "        'entertainment', 'drama', 'show', 'artist', 'singer', 'star', 'TV',\n",
    "        'Netflix', 'performance', 'premiere', 'award', 'festival', 'viral',\n",
    "        'album', 'song', 'talent', 'meme', 'trending', 'Astro', 'Media Prima'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Load your CSV data\n",
    "df = pd.read_csv('tweets_output_with_sentiment.csv')\n",
    "\n",
    "# Initialize Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Define function to categorize tweets based on keywords\n",
    "def categorize_tweet(tweet_text):\n",
    "    # Ensure tweet_text is a string\n",
    "    if isinstance(tweet_text, str):  # Check if tweet_text is a string\n",
    "        tweet_text = tweet_text.lower()  # Make the text case-insensitive\n",
    "        for category, keywords in CATEGORY_KEYWORDS.items():\n",
    "            if any(keyword in tweet_text for keyword in keywords):  # Check if any keyword matches\n",
    "                return category\n",
    "    return 'MalaysiaNewsTopic'  # Default topic if no category is found or invalid tweet\n",
    "\n",
    "# Iterate and send data to the appropriate Kafka topic\n",
    "for index, row in df.iterrows():\n",
    "    tweet_data = {\n",
    "        'user_id': row['User ID'],\n",
    "        'name': row['Name'],\n",
    "        'followers_count': row['Followers Count'],\n",
    "        'tweet_text': row['Tweet'],\n",
    "        'Location': row['Location'], \n",
    "        'Tweet Time': row['Tweet Time'], \n",
    "        'Friends Count': row['Friends Count'], \n",
    "        'sentiment': row['Sentiment']\n",
    "    }\n",
    "\n",
    "    # Categorize the tweet into the correct topic\n",
    "    topic = categorize_tweet(row['Tweet'])\n",
    "\n",
    "    # Send each row to the appropriate Kafka topic\n",
    "    producer.send(topic, value=tweet_data)\n",
    "\n",
    "    # Print confirmation (optional)\n",
    "    print(f\"Sent tweet from {row['Name']} to {topic}\")\n",
    "\n",
    "# Close the producer\n",
    "producer.flush()\n",
    "producer.close()\n",
    "print(\"All tweets have been sent to Kafka\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be230fc9-91a0-4d1c-891a-c9826b985cf5",
   "metadata": {},
   "source": [
    "# write into kafka consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e2f98-df13-4611-ad08-fee88b104ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Function to consume messages from the selected topic\n",
    "def consume_messages_from_topic(selected_topic):\n",
    "    # Initialize Kafka consumer for the selected topic\n",
    "    consumer = KafkaConsumer(\n",
    "        selected_topic,  # The topic to verify\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        auto_offset_reset='earliest',  # Start from the beginning of the topic\n",
    "        group_id='verification_group',  # A unique consumer group for this verification\n",
    "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "        consumer_timeout_ms=10000  # Exit after 10 seconds of no new messages\n",
    "    )\n",
    "\n",
    "    print(f\"Checking messages in {selected_topic}...\")\n",
    "    message_count = 0\n",
    "\n",
    "    # Try to consume messages\n",
    "    for message in consumer:\n",
    "        message_count += 1\n",
    "        print(f\"Message {message_count}: {message.value}\")\n",
    "        \n",
    "        # Optional: limit the number of messages to display\n",
    "        if message_count >= 5:\n",
    "            print(f\"Showing first 5 messages. Topic contains more messages...\")\n",
    "            break\n",
    "\n",
    "    # Close the consumer\n",
    "    consumer.close()\n",
    "\n",
    "    # Check if messages were found\n",
    "    if message_count > 0:\n",
    "        print(f\"\\n✅ Topic verification successful! Found {message_count} messages in {selected_topic}.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ No messages found in {selected_topic}. Please check your producer code or Kafka setup.\")\n",
    "\n",
    "# Allow user to choose the topic\n",
    "available_topics = [\n",
    "    'PoliticsNewsTopic',\n",
    "    'BusinessNewsTopic',\n",
    "    'SportsNewsTopic',\n",
    "    'EntertainmentNewsTopic',\n",
    "    'MalaysiaNewsTopic'\n",
    "]\n",
    "\n",
    "print(\"Available Topics to verify:\")\n",
    "for i, topic in enumerate(available_topics, 1):\n",
    "    print(f\"{i}. {topic}\")\n",
    "\n",
    "# Ask the user to select a topic\n",
    "try:\n",
    "    topic_choice = int(input(\"\\nEnter the number of the topic you want to verify: \"))\n",
    "    if 1 <= topic_choice <= len(available_topics):\n",
    "        selected_topic = available_topics[topic_choice - 1]\n",
    "        consume_messages_from_topic(selected_topic)\n",
    "    else:\n",
    "        print(\"\\n❌ Invalid selection. Please choose a valid topic number.\")\n",
    "except ValueError:\n",
    "    print(\"\\n❌ Invalid input. Please enter a number.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd09f21-7648-40e9-b3e2-e447949bdbea",
   "metadata": {},
   "source": [
    "# store data to HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65fc9e-eacb-4632-850d-98188a55bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'MalaysiaNewsTopic',  # Topic name\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    group_id='tweet_consumer_group',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Initialize HDFS client\n",
    "hdfs_client = InsecureClient('http://localhost:9870', user='hduser')\n",
    "\n",
    "# Specify the HDFS directory\n",
    "hdfs_directory = '/user/hduser/raw_tweets/'\n",
    "\n",
    "def write_to_hdfs(data):\n",
    "    try:\n",
    "        # Generate unique filename with timestamp and UUID\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        unique_id = str(uuid.uuid4())[:8]  # Use first 8 chars of UUID for brevity\n",
    "        file_name = f\"tweets_raw_{timestamp}_{unique_id}.json\"\n",
    "        \n",
    "        # Full path to HDFS\n",
    "        hdfs_path = f\"{hdfs_directory}/{file_name}\"\n",
    "        \n",
    "        # Convert the data to JSON\n",
    "        data_str = json.dumps(data, ensure_ascii=False)\n",
    "        \n",
    "        # Write the data to HDFS\n",
    "        with hdfs_client.write(hdfs_path, encoding='utf-8') as writer:\n",
    "            writer.write(data_str)\n",
    "            \n",
    "        print(f\"Successfully wrote to HDFS: {hdfs_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to HDFS: {e}\")\n",
    "        return False\n",
    "\n",
    "# Batch processing option\n",
    "def write_batch_to_hdfs(batch_data, batch_size=10):\n",
    "    if not batch_data:\n",
    "        return True\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = str(uuid.uuid4())[:8]\n",
    "    file_name = f\"tweets_batch_{timestamp}_{unique_id}_{batch_size}.json\"\n",
    "    hdfs_path = f\"{hdfs_directory}/{file_name}\"\n",
    "    \n",
    "    data_str = json.dumps(batch_data, ensure_ascii=False)\n",
    "    \n",
    "    with hdfs_client.write(hdfs_path, encoding='utf-8') as writer:\n",
    "        writer.write(data_str)\n",
    "    \n",
    "    print(f\"Successfully wrote batch of {len(batch_data)} tweets to HDFS\")\n",
    "    return True\n",
    "\n",
    "# Main processing loop\n",
    "try:\n",
    "    # For batch processing\n",
    "    batch = []\n",
    "    batch_size = 10\n",
    "    \n",
    "    for message in consumer:\n",
    "        tweet_data = message.value\n",
    "        \n",
    "        # Ensure tweet data contains the fields we need\n",
    "        tweet_data = {\n",
    "            'user_id': tweet_data['user_id'],\n",
    "            'name': tweet_data['name'],\n",
    "            'followers_count': tweet_data['followers_count'],\n",
    "            'tweet_text': tweet_data['tweet_text'],\n",
    "            'sentiment': tweet_data.get('sentiment', 'Unknown')  # Include sentiment if available\n",
    "        }\n",
    "        \n",
    "        print(f\"Received tweet data: {tweet_data}\")\n",
    "        \n",
    "        # Write individual tweet data to HDFS\n",
    "        write_to_hdfs(tweet_data)\n",
    "        \n",
    "        # Batch processing logic\n",
    "        batch.append(tweet_data)\n",
    "        if len(batch) >= batch_size:\n",
    "            write_batch_to_hdfs(batch, batch_size)\n",
    "            batch = []  # Reset batch after writing\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping consumer...\")\n",
    "finally:\n",
    "    consumer.close()\n",
    "    print(\"Consumer closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7f24b-5c02-44de-b9bb-860959c559c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont RUN !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3fc1c-a523-4d67-9045-eacccd3c660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data exits or not\n",
    "# hadoop fs -ls /user/hduser/raw_tweets/  \n",
    "# hadoop fs -cat /user/hduser/raw_tweets//tweets_raw_20250314_171804_7933c577.json\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
